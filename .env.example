# LLM Provider Selection (ollama or chatgroq)
LLM_PROVIDER=ollama

# ChatGroq API Configuration
GROQ_API_KEY=your_groq_api_key_here
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_MODEL_NAME=llama3-8b-8192

# Ollama Configuration (for local LLM hosting)
# Use localhost for direct access, or http://ollama:11434 for Docker Compose
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_NAME=llama3.2:1b

# Backward Compatibility (falls back to GROQ_MODEL_NAME)
MODEL_NAME=llama3-8b-8192

# LLM Configuration
TEMPERATURE=0.7
MAX_TOKENS=1024

# Memory Configuration
MEMORY_TTL_SECONDS=3600
MAX_CACHE_SIZE=1000